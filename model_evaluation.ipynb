{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11903193,"sourceType":"datasetVersion","datasetId":7482501}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers diffusers imageio scipy timm accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T20:25:37.531396Z","iopub.execute_input":"2026-01-06T20:25:37.531689Z","iopub.status.idle":"2026-01-06T20:25:42.121773Z","shell.execute_reply.started":"2026-01-06T20:25:37.531653Z","shell.execute_reply":"2026-01-06T20:25:42.120789Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport cv2\nimport glob\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom scipy.io import loadmat\nfrom diffusers import MarigoldDepthPipeline\nfrom transformers import AutoImageProcessor, AutoModelForDepthEstimation, ZoeDepthForDepthEstimation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T20:25:46.580809Z","iopub.execute_input":"2026-01-06T20:25:46.581574Z","iopub.status.idle":"2026-01-06T20:26:21.900426Z","shell.execute_reply.started":"2026-01-06T20:25:46.581536Z","shell.execute_reply":"2026-01-06T20:26:21.899787Z"}},"outputs":[{"name":"stderr","text":"2026-01-06 20:26:07.036425: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767731167.234237      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767731167.292167      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767731167.769577      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767731167.769623      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767731167.769626      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767731167.769628      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"IBIMS_PATH = \"/kaggle/input/ibims-1/iBims-1\"\n\nDEVICE = torch.device(\"cuda\") if torch.cuda.is_available else torch.device(\"cpu\")\nprint(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T20:26:25.120882Z","iopub.execute_input":"2026-01-06T20:26:25.121953Z","iopub.status.idle":"2026-01-06T20:26:25.127010Z","shell.execute_reply.started":"2026-01-06T20:26:25.121920Z","shell.execute_reply":"2026-01-06T20:26:25.126173Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# https://www.asg.ed.tum.de/lmf/ibims1/\n\nclass IBimsLoader:\n    def __init__(self, root_dir=IBIMS_PATH):\n        self.rgb_files = sorted(glob.glob(os.path.join(root_dir, \"rgb\", \"*.png\")))\n        self.depth_files = sorted(glob.glob(os.path.join(root_dir, \"ibims1_core_mat\", \"*.mat\")))\n        \n        if len(self.rgb_files) != len(self.depth_files):\n            print(\"Hmm something is wrong with the dataset...\")\n\n    def __len__(self):\n        return len(self.rgb_files)\n\n    def get_item(self, idx):\n        img_path = self.rgb_files[idx]\n        img = cv2.imread(img_path)\n        #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        depth_path = self.depth_files[idx]\n        mat_data = loadmat(depth_path)\n        \n        #key = [k for k in mat_data.keys() if not k.startswith('_')][0]\n        #data_struct = mat_data[key][0, 0]\n        \n        #gt_depth = data_struct['depth']\n        #gt_depth = gt_depth.astype(np.float32)\n        #invalid_mask = data_struct['mask_invalid'].astype(bool)\n\n        # load raw depth map\n        gt_depth = mat_data['data']['depth'][0][0]\n\n        mask = gt_depth > 0.001\n        \n        return img, gt_depth, mask, os.path.basename(img_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T20:31:50.357887Z","iopub.execute_input":"2026-01-06T20:31:50.358715Z","iopub.status.idle":"2026-01-06T20:31:50.364745Z","shell.execute_reply.started":"2026-01-06T20:31:50.358681Z","shell.execute_reply":"2026-01-06T20:31:50.363976Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def get_absrel(gt, pred):\n    return np.mean(np.abs(gt-pred)/gt)\n\ndef get_delta(gt, pred, exponent=1):\n    inlier = np.maximum((gt/pred), (pred/gt))\n    return np.mean(inlier < 1.25**exponent)\n\ndef align_depth_least_square(gt_arr, pred_arr, valid_mask_arr):\n    ori_shape = pred_arr.shape\n\n    gt = gt_arr.squeeze()  # [H, W]\n    pred = pred_arr.squeeze()\n    valid_mask = valid_mask_arr.squeeze()\n    gt_masked = gt[valid_mask].reshape((-1, 1))\n    pred_masked = pred[valid_mask].reshape((-1, 1))\n\n    # numpy solver\n    _ones = np.ones_like(pred_masked)\n    A = np.concatenate([pred_masked, _ones], axis=-1)\n    X = np.linalg.lstsq(A, gt_masked, rcond=None)[0]\n    scale, shift = X\n\n    aligned_pred = pred_arr * scale + shift\n\n    # restore dimensions\n    aligned_pred = aligned_pred.reshape(ori_shape)\n\n    return aligned_pred, scale, shift","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T20:32:06.494313Z","iopub.execute_input":"2026-01-06T20:32:06.494906Z","iopub.status.idle":"2026-01-06T20:32:06.500921Z","shell.execute_reply.started":"2026-01-06T20:32:06.494874Z","shell.execute_reply":"2026-01-06T20:32:06.500115Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class DepthMetrics:\n    def __init__(self):\n        pass\n\n    def align_scale_shift(self, pred, target):\n        \"\"\"\n        MiDaS paper - p.5\n        Aligns prediction to target using Least Squares (Scale & Shift).\n        Used for Relative Depth Models (MiDaS, Depth Anything Base).\n        Formula: s*, t* = argmin || s * pred + t - target ||^2\n        \"\"\"\n        mask = (target > 0)\n        target_masked = target[mask]\n        pred_masked = pred[mask]\n\n        if len(pred_masked) < 10: return pred, 1.0, 0.0\n        \n        slope, intercept = np.polyfit(pred_masked, target_masked, 1)\n        \n        pred_aligned = pred * slope + intercept\n        return pred_aligned, slope, intercept\n\n    def align_median(self, pred, target):\n        \"\"\"\n        Simple Median Scaling. often used for Metric models to correct global scale drift.\n        \"\"\"\n        mask = (target > 0)\n        scale = np.median(target[mask]) / np.median(pred[mask])\n        return pred * scale\n\n    def compute(self, pred, target, invalid_mask=None, align_type=\"none\"):\n        \"\"\"\n        Calculates: AbsRel, RMSE, Delta1 (a1).\n        align_type: 'none' (for Metric League), 'least_squares' (for Relative League)\n        \"\"\"\n        valid_mask = invalid_mask if invalid_mask is not None else np.ones_like(target, dtype=bool)\n        \n        # GT > 0.1 ensures we don't divide by tiny numbers\n        # GT < 80.0 removes infinite sky/sensor errors\n        valid_mask = valid_mask & (target > 0.1) & (target < 80.0)\n        \n        valid_mask = valid_mask & (~np.isnan(target)) & (~np.isnan(pred))\n        if valid_mask.sum() == 0: return None\n\n        pred_valid = pred[valid_mask]\n        target_valid = target[valid_mask]\n\n        if align_type == \"least_squares\":\n            pred_valid, _, _ = self.align_scale_shift(pred_valid, target_valid)\n        elif align_type == \"median\":\n            scale = np.median(target_valid) / np.median(pred_valid)\n            pred_valid = pred_valid * scale\n\n        pred_valid = np.clip(pred_valid, 0.001, 80.0)\n\n        # AbsRel: |pred - gt| / gt\n        abs_rel = np.mean(np.abs(pred_valid - target_valid) / target_valid)\n\n        # RMSE\n        rmse = np.sqrt(np.mean((pred_valid - target_valid) ** 2))\n\n        # Delta Accuracy: max(pred/gt, gt/pred) < 1.25\n        thresh = np.maximum((target_valid / pred_valid), (pred_valid / target_valid))\n        a1 = (thresh < 1.25).mean()\n\n        return {\"abs_rel\": abs_rel, \"rmse\": rmse, \"a1\": a1}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T19:36:25.692633Z","iopub.execute_input":"2026-01-05T19:36:25.692921Z","iopub.status.idle":"2026-01-05T19:36:25.701054Z","shell.execute_reply.started":"2026-01-05T19:36:25.692895Z","shell.execute_reply":"2026-01-05T19:36:25.700513Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"class TransformerModelWrapper:\n    def __init__(self, choice):\n        self.processor = AutoImageProcessor.from_pretrained(choice)\n        self.model = AutoModelForDepthEstimation.from_pretrained(choice).to(DEVICE)\n\n    def infer(self, image_path):\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        inputs = self.processor(images=image, return_tensors=\"pt\").to(DEVICE)\n        \n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            predicted_depth = outputs.predicted_depth\n        \n        # Resize to original image size\n        prediction = torch.nn.functional.interpolate(\n            predicted_depth.unsqueeze(1),\n            size=image.shape[:2],\n            mode=\"bicubic\",\n            align_corners=False,\n        ).squeeze().cpu().numpy()\n        \n        return prediction\n\nclass ModelRunner:\n    def __init__(self, device=\"cuda\"):\n        self.device = device\n        self.models = {}\n        self.processors = {}\n    \n    def load_depth_anything_v2(self, variant=\"metric\"):\n        if variant == \"metric\":\n            mid = \"depth-anything/Depth-Anything-V2-Metric-Indoor-Large-hf\"\n        else:\n            mid = \"depth-anything/Depth-Anything-V2-Small-hf\"\n            \n        print(f\"Loading {mid}...\")\n        self.processors[\"da_v2\"] = AutoImageProcessor.from_pretrained(mid)\n        self.models[\"da_v2\"] = AutoModelForDepthEstimation.from_pretrained(mid).to(self.device)\n    \n    def load_zoedepth(self):\n        print(\"Loading ZoeDepth...\")\n        mid = \"intel-isl/ZoeD_M12_N\"\n        self.processors[\"zoe\"] = AutoImageProcessor.from_pretrained(mid)\n        self.models[\"zoe\"] = ZoeDepthForDepthEstimation.from_pretrained(mid).to(self.device)\n\n    def load_marigold(self):\n        print(\"Loading Marigold (Diffusion)...\")\n        pipe = MarigoldDepthPipeline.from_pretrained(\n            \"prs-eth/marigold-v1-0\", torch_dtype=torch.float16\n        )\n        pipe.to(self.device)\n        self.models[\"marigold\"] = pipe\n\n    def infer(self, model_name, image_path):\n        \"\"\"\n        Generic inference wrapper\n        \"\"\"\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if model_name == \"marigold\":\n            from PIL import Image\n            pil_img = Image.fromarray(image)\n            pipe_out = self.models[\"marigold\"](pil_img, num_inference_steps=10) # 10 is fast, 50 is precise\n            depth = pipe_out.depth_np\n            return depth\n\n        processor = self.processors[model_name]\n        model = self.models[model_name]\n        \n        inputs = processor(images=image, return_tensors=\"pt\").to(self.device)\n        \n        with torch.no_grad():\n            outputs = model(**inputs)\n            predicted_depth = outputs.predicted_depth\n        \n        # Resize to original image size\n        prediction = torch.nn.functional.interpolate(\n            predicted_depth.unsqueeze(1),\n            size=image.shape[:2],\n            mode=\"bicubic\",\n            align_corners=False,\n        ).squeeze().cpu().numpy()\n        \n        return prediction","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T20:39:39.343369Z","iopub.execute_input":"2026-01-06T20:39:39.344125Z","iopub.status.idle":"2026-01-06T20:39:39.356874Z","shell.execute_reply.started":"2026-01-06T20:39:39.344097Z","shell.execute_reply":"2026-01-06T20:39:39.355998Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def get_final_results(results):\n    if len(results) > 0:\n        print(\"\\n\" + \"=\"*40)\n        \n        avg_results = {}\n        for key in results[0].keys():\n            avg_results[key] = np.mean([res[key] for res in results])\n    \n        print(f\"AbsRel (Lower is better):  {avg_results['abs_rel']:.4f}\")\n        print(f\"RMSE   (Lower is better):  {avg_results['rmse']:.4f}\")\n        print(f\"Delta1 (Higher is better): {avg_results['a1']:.4f}\")\n        print(\"=\"*40)\n        return avg_results\n    else:\n        print(\"No valid results found.\")\n    return None\n\ndef run_transformer_over_dataset(model, dataset, metrics_calc, align_type=\"none\", debug=False):\n    results = []\n    for i in tqdm(range(len(dataset))):\n        img, gt, mask, name = dataset.get_item(i)\n        #print(img.shape, gt.shape, name)\n        image_path = f\"{IBIMS_PATH}/rgb/{name}\"\n        \n        prediction = dav2_base.infer(image_path)\n        metrics = metrics_calc.compute(prediction, gt, invalid_mask=mask, align_type=align_type)\n        if metrics is not None:\n            results.append(metrics)\n            if debug: \n                print(f\"{name}\\tAbsRel: {metrics['abs_rel']:.3f}\\tRMSE: {metrics['rmse']:.3f}\\tDelta1: {metrics['a1']:.3f}\")\n                print(f\"Pred Median: {np.median(prediction):.2f}, GT Median: {np.median(gt):.2f}\")\n        else:\n            print(f\"Something went wrong with {name}. Skipping...\")\n    \n    return get_final_results(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T19:27:53.362928Z","iopub.execute_input":"2026-01-05T19:27:53.363338Z","iopub.status.idle":"2026-01-05T19:27:53.370080Z","shell.execute_reply.started":"2026-01-05T19:27:53.363310Z","shell.execute_reply":"2026-01-05T19:27:53.369548Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"dataset = IBimsLoader()\nmetrics_calc = DepthMetrics()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T19:36:31.275424Z","iopub.execute_input":"2026-01-05T19:36:31.275734Z","iopub.status.idle":"2026-01-05T19:36:31.283535Z","shell.execute_reply.started":"2026-01-05T19:36:31.275707Z","shell.execute_reply":"2026-01-05T19:36:31.282984Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"img, gt, invalid_mask, name = dataset.get_item(12)\nprint(np.where(invalid_mask == True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T19:33:15.642296Z","iopub.execute_input":"2026-01-05T19:33:15.642566Z","iopub.status.idle":"2026-01-05T19:33:15.685054Z","shell.execute_reply.started":"2026-01-05T19:33:15.642544Z","shell.execute_reply":"2026-01-05T19:33:15.684503Z"}},"outputs":[{"name":"stdout","text":"(array([  0,   0,   0, ..., 479, 479, 479]), array([  0,   1,   2, ..., 637, 638, 639]))\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"# Check if dataset gt is in mm\nfor i in range(len(dataset)):\n    img, gt, invalid_mask, name = dataset.get_item(i)\n    if np.median(gt) > 100:\n        print(\"maybe that measurement is in millimeters... needs to change!\")\n    #print(np.where(invalid_mask == False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T19:35:30.626753Z","iopub.execute_input":"2026-01-05T19:35:30.627417Z","iopub.status.idle":"2026-01-05T19:35:34.403922Z","shell.execute_reply.started":"2026-01-05T19:35:30.627382Z","shell.execute_reply":"2026-01-05T19:35:34.403281Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"dav2_base = TransformerModelWrapper(\"depth-anything/Depth-Anything-V2-Small-hf\")\nrun_transformer_over_dataset(dav2_base, dataset, metrics_calc, align_type=\"least_squares\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T19:36:38.243225Z","iopub.execute_input":"2026-01-05T19:36:38.243932Z","iopub.status.idle":"2026-01-05T19:36:52.215879Z","shell.execute_reply.started":"2026-01-05T19:36:38.243900Z","shell.execute_reply":"2026-01-05T19:36:52.215325Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [00:13<00:00,  7.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n========================================\nAbsRel (Lower is better):  0.1192\nRMSE   (Lower is better):  0.5311\nDelta1 (Higher is better): 0.8536\n========================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"{'abs_rel': np.float64(0.11924478010784752),\n 'rmse': np.float64(0.5311465352130827),\n 'a1': np.float64(0.8535762321108398)}"},"metadata":{}}],"execution_count":79},{"cell_type":"code","source":"dav2_metric = TransformerModelWrapper(\"depth-anything/Depth-Anything-V2-Metric-Indoor-Large-hf\")\nrun_transformer_over_dataset(dav2_metric, dataset, metrics_calc, align_type=\"none\", debug=False)\n\n#print(\"\\nLet's apply some scaling to test what changes\")\n#run_transformer_over_dataset(dav2_metric, dataset, metrics_calc, align_type=\"median\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T19:37:14.843982Z","iopub.execute_input":"2026-01-05T19:37:14.844754Z","iopub.status.idle":"2026-01-05T19:37:27.126174Z","shell.execute_reply.started":"2026-01-05T19:37:14.844722Z","shell.execute_reply":"2026-01-05T19:37:27.125482Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [00:11<00:00,  8.81it/s]","output_type":"stream"},{"name":"stdout","text":"\n========================================\nAbsRel (Lower is better):  0.9662\nRMSE   (Lower is better):  3.0890\nDelta1 (Higher is better): 0.1500\n========================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"{'abs_rel': np.float32(0.96617407),\n 'rmse': np.float32(3.0889754),\n 'a1': np.float64(0.14998329868036855)}"},"metadata":{}}],"execution_count":81},{"cell_type":"code","source":"run_transformer_over_dataset(dav2_metric, dataset, metrics_calc, align_type=\"median\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T19:37:50.993439Z","iopub.execute_input":"2026-01-05T19:37:50.993742Z","iopub.status.idle":"2026-01-05T19:38:02.660490Z","shell.execute_reply.started":"2026-01-05T19:37:50.993716Z","shell.execute_reply":"2026-01-05T19:38:02.659922Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [00:11<00:00,  8.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n========================================\nAbsRel (Lower is better):  1.2479\nRMSE   (Lower is better):  4.2113\nDelta1 (Higher is better): 0.2292\n========================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"{'abs_rel': np.float32(1.2478518),\n 'rmse': np.float32(4.2112966),\n 'a1': np.float64(0.22922321199906034)}"},"metadata":{}}],"execution_count":82},{"cell_type":"code","source":"dataset = IBimsLoader()\ndav2_base = TransformerModelWrapper(\"depth-anything/Depth-Anything-V2-Large-hf\")\n\nabsrel_list = []\ndelta_list = []\n\nfor i in tqdm(range(len(dataset))):\n    img, gt, mask, name = dataset.get_item(i)\n    image_path = f\"{IBIMS_PATH}/rgb/{name}\"\n        \n    prediction = dav2_base.infer(image_path)\n    depth_trans, scale, shift = align_depth_least_square(gt, prediction, mask)\n\n    absrel = get_absrel(gt[mask], depth_trans[mask])\n    delta = get_delta(gt[mask], depth_trans[mask], 1)\n\n    absrel_list.append(absrel)\n    delta_list.append(delta)\n\nprint(f\"Average Abs Rel: {np.mean(absrel_list)}\")\nprint(f\"Average d_1: {np.mean(delta_list)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T20:40:53.132924Z","iopub.execute_input":"2026-01-06T20:40:53.133374Z","iopub.status.idle":"2026-01-06T20:41:42.124065Z","shell.execute_reply.started":"2026-01-06T20:40:53.133344Z","shell.execute_reply":"2026-01-06T20:41:42.123463Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/775 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12c2c6a4e7734f6c86490434b69381e5"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7236c6d6b114bc7b8bcf9f14a7c2429"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aebe3951b3244db8e346af3b85549b7"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 100/100 [00:43<00:00,  2.32it/s]","output_type":"stream"},{"name":"stdout","text":"Average Abs Rel: 0.1252612876460376\nAverage d_1: 0.8572297850934841\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"dataset = IBimsLoader()\ndav2_metric = TransformerModelWrapper(\"depth-anything/Depth-Anything-V2-Metric-Indoor-Large-hf\")\n\nabsrel_list = []\ndelta_list = []\n\nfor i in tqdm(range(len(dataset))):\n    img, gt, mask, name = dataset.get_item(i)\n    image_path = f\"{IBIMS_PATH}/rgb/{name}\"\n        \n    prediction = dav2_metric.infer(image_path)\n    \n    absrel = get_absrel(gt[mask], prediction[mask])\n    delta = get_delta(gt[mask], prediction[mask], 1)\n\n    absrel_list.append(absrel)\n    delta_list.append(delta)\n\nprint(f\"Average Abs Rel: {np.mean(absrel_list)}\")\nprint(f\"Average d_1: {np.mean(delta_list)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T20:45:34.230034Z","iopub.execute_input":"2026-01-06T20:45:34.230641Z","iopub.status.idle":"2026-01-06T20:46:12.158404Z","shell.execute_reply.started":"2026-01-06T20:45:34.230609Z","shell.execute_reply":"2026-01-06T20:46:12.157806Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [00:36<00:00,  2.71it/s]","output_type":"stream"},{"name":"stdout","text":"Average Abs Rel: 0.1264849104849234\nAverage d_1: 0.886137028246952\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":13}]}