{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2002504,"sourceType":"datasetVersion","datasetId":1198025},{"sourceId":11903193,"sourceType":"datasetVersion","datasetId":7482501}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport glob\nimport torch\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom scipy.io import loadmat\nfrom scipy.stats import spearmanr\nfrom diffusers import MarigoldDepthPipeline\nfrom transformers import AutoImageProcessor, AutoModelForDepthEstimation, ZoeDepthForDepthEstimation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T21:59:20.550283Z","iopub.execute_input":"2026-01-09T21:59:20.550934Z","iopub.status.idle":"2026-01-09T21:59:55.194532Z","shell.execute_reply.started":"2026-01-09T21:59:20.550904Z","shell.execute_reply":"2026-01-09T21:59:55.193751Z"}},"outputs":[{"name":"stderr","text":"2026-01-09 21:59:40.873372: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767995981.061379      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767995981.112813      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767995981.565590      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767995981.565630      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767995981.565633      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767995981.565635      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"IBIMS_PATH = \"/kaggle/input/ibims-1/iBims-1\"\nNYU_PATH = \"/kaggle/input/nyu-depth-v2/nyu_data/data/nyu2_test\"\n\nDEVICE = torch.device(\"cuda\") if torch.cuda.is_available else torch.device(\"cpu\")\nprint(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T21:59:58.317257Z","iopub.execute_input":"2026-01-09T21:59:58.317838Z","iopub.status.idle":"2026-01-09T21:59:58.322519Z","shell.execute_reply.started":"2026-01-09T21:59:58.317810Z","shell.execute_reply":"2026-01-09T21:59:58.321645Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# https://www.asg.ed.tum.de/lmf/ibims1/\n\nclass IBimsLoader:\n    def __init__(self, root_dir=IBIMS_PATH):\n        self.rgb_files = sorted(glob.glob(os.path.join(root_dir, \"rgb\", \"*.png\")))\n        self.depth_files = sorted(glob.glob(os.path.join(root_dir, \"ibims1_core_mat\", \"*.mat\")))\n        \n        if len(self.rgb_files) != len(self.depth_files):\n            print(\"Hmm something is wrong with the dataset...\")\n\n    def __len__(self):\n        return len(self.rgb_files)\n\n    def get_item(self, idx):\n        img_path = self.rgb_files[idx]\n        img = cv2.imread(img_path)\n        #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        depth_path = self.depth_files[idx]\n        mat_data = loadmat(depth_path)\n\n        gt_depth = mat_data['data']['depth'][0][0]\n\n        mask = gt_depth > 0.001\n        \n        return img, gt_depth, mask,img_path\n\nclass NyuLoader:\n    def __init__(self, root_dir=NYU_PATH, samples_size=100, seed=42):\n        self.samples = []\n        \n        self.rgb_files = sorted(glob.glob(os.path.join(root_dir, \"*_colors.png\"), recursive=True))\n        print(f\"Found {len(self.rgb_files)} RGB candidates. Matching with Depth...\")\n\n        for rgb_path in self.rgb_files:\n            depth_path = rgb_path.replace(\"_colors.png\", \"_depth.png\")\n            if os.path.exists(depth_path):\n                self.samples.append((rgb_path, depth_path))\n\n        if samples_size is not None and len(self.samples) > samples_size:\n            random.seed(seed)\n            self.samples = random.sample(self.samples, samples_size)\n    \n    def __len__(self):\n        return len(self.samples)\n\n    def get_item(self, idx):\n        img_path, depth_path = self.samples[idx]\n        \n        img = cv2.imread(img_path)\n        depth_png = cv2.imread(depth_path, -1)\n        if depth_png is None:\n            raise ValueError(f\"Failed to load depth: {depth_path}\")\n\n        gt_depth = depth_png.astype(np.float32) / 1000.0\n        mask = gt_depth > 0.001\n\n        return img, gt_depth, mask, img_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T22:00:01.231007Z","iopub.execute_input":"2026-01-09T22:00:01.231721Z","iopub.status.idle":"2026-01-09T22:00:01.240269Z","shell.execute_reply.started":"2026-01-09T22:00:01.231690Z","shell.execute_reply":"2026-01-09T22:00:01.239689Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"#  https://huggingface.co/blog/Isayoften/monocular-depth-estimation-guide\n\ndef get_absrel(gt, pred):\n    return np.mean(np.abs(gt-pred)/gt)\n\ndef get_delta(gt, pred, exponent=1):\n    inlier = np.maximum((gt/pred), (pred/gt))\n    return np.mean(inlier < 1.25**exponent)\n\ndef get_silog(gt, pred):\n    \"\"\"\n    Computes Scale Invariant Logarithmic Error.\n    Lower is better.\n    \"\"\"\n    pred = np.maximum(pred, 1e-5)\n    gt = np.maximum(gt, 1e-5)\n\n    err = np.log(pred) - np.log(gt)\n\n    silog = np.sqrt(np.mean(err ** 2) - (np.mean(err)) ** 2)\n    \n    return silog * 100\n\ndef align_depth_least_square(gt_arr, pred_arr, valid_mask_arr):\n    ori_shape = pred_arr.shape\n\n    gt = gt_arr.squeeze()  # [H, W]\n    pred = pred_arr.squeeze()\n    valid_mask = valid_mask_arr.squeeze()\n    gt_masked = gt[valid_mask].reshape((-1, 1))\n    pred_masked = pred[valid_mask].reshape((-1, 1))\n\n    # numpy solver\n    _ones = np.ones_like(pred_masked)\n    A = np.concatenate([pred_masked, _ones], axis=-1)\n    X = np.linalg.lstsq(A, gt_masked, rcond=None)[0]\n    scale, shift = X\n\n    aligned_pred = pred_arr * scale + shift\n\n    # restore dimensions\n    aligned_pred = aligned_pred.reshape(ori_shape)\n\n    return aligned_pred, scale, shift","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T22:00:04.377664Z","iopub.execute_input":"2026-01-09T22:00:04.377976Z","iopub.status.idle":"2026-01-09T22:00:04.384949Z","shell.execute_reply.started":"2026-01-09T22:00:04.377947Z","shell.execute_reply":"2026-01-09T22:00:04.384396Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class ModelWrapper:\n    def __init__(self, choice):\n        self.choice = choice\n        \n        if \"marigold\" in choice.lower():\n            self.model = MarigoldDepthPipeline.from_pretrained(choice, variant=\"fp16\").to(DEVICE)\n            self.model.set_progress_bar_config(disable=True)\n            print(\"found marigold model\")\n        elif \"zoedepth\" in choice.lower():\n            self.processor = AutoImageProcessor.from_pretrained(choice)\n            self.model = ZoeDepthForDepthEstimation.from_pretrained(choice).to(DEVICE)\n            print(\"found zoedepth model\")\n        else:\n            self.processor = AutoImageProcessor.from_pretrained(choice)\n            self.model = AutoModelForDepthEstimation.from_pretrained(choice).to(DEVICE)\n\n    def infer(self, image_path, marigold_steps=4):\n        if \"marigold\" in self.choice.lower():\n            image = Image.open(image_path).convert(\"RGB\")\n            w0, h0 = image.size\n            \n            with torch.no_grad():\n                pipe_out = self.model(\n                    image, \n                    num_inference_steps=marigold_steps, \n                    output_type=\"pt\"\n                )\n            prediction = pipe_out.prediction\n\n            # Check if resize is needed\n            if prediction.shape[-2:] != (h0, w0):\n                prediction = torch.nn.functional.interpolate(\n                    prediction,\n                    size=(h0, w0),\n                    mode=\"bicubic\",\n                    align_corners=False\n                )\n\n            return prediction.squeeze().cpu().numpy()\n\n        # Transformer-based models inference\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        inputs = self.processor(images=image, return_tensors=\"pt\").to(DEVICE)\n        \n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            predicted_depth = outputs.predicted_depth\n        \n        # Resize to original image size\n        prediction = torch.nn.functional.interpolate(\n            predicted_depth.unsqueeze(1),\n            size=image.shape[:2],\n            mode=\"bicubic\",\n            align_corners=False,\n        )\n        \n        return prediction.squeeze().cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T22:00:16.991483Z","iopub.execute_input":"2026-01-09T22:00:16.991792Z","iopub.status.idle":"2026-01-09T22:00:17.000168Z","shell.execute_reply.started":"2026-01-09T22:00:16.991765Z","shell.execute_reply":"2026-01-09T22:00:16.999379Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Check if dataset gt is in mm\n# dataset = ...\nfor i in range(len(dataset)):\n    _, gt, _, _ = dataset.get_item(i)\n    if np.median(gt) > 100:\n        print(\"maybe that measurement is in millimeters... needs to change!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T19:35:30.626753Z","iopub.execute_input":"2026-01-05T19:35:30.627417Z","iopub.status.idle":"2026-01-05T19:35:34.403922Z","shell.execute_reply.started":"2026-01-05T19:35:30.627382Z","shell.execute_reply":"2026-01-05T19:35:34.403281Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"ibims_loader = IBimsLoader()\nnyu_loader = NyuLoader()\n\nclass ModelConfig:\n    def __init__(self, model_choice, display_name, relative=True, ibims=ibims_loader, nyu=nyu_loader):\n        self.model = ModelWrapper(model_choice)\n        self.display_name = display_name\n        self.relative = relative\n        self.ibims = ibims\n        self.nyu = nyu\n        self.absrel = {}\n        self.d1 = {}\n        self.silog = {}\n\n    def set_results(self, dataset, absrel, d1, silog):\n        self.absrel[dataset] = absrel;\n        self.d1[dataset] = d1\n        self.silog[dataset] = silog\n\n    def get_absrel_result(self, dataset):\n        return self.absrel[dataset]\n\n    def get_d1_result(self, dataset):\n        return self.d1[dataset]\n\n    def get_silog_result(self, dataset):\n        return self.silog[dataset]\n\n    def __check_if_disparity(self, dataset, num_samples=5):\n        \"\"\"\n        Checks if model output is Disparity (needs 1/x) or Depth.\n        Returns True if the output is Disparity (Negative Correlation with GT).\n        \"\"\"\n        correlations = []\n        indices = np.linspace(0, len(dataset)-1, num_samples, dtype=int)\n        \n        for i in indices:\n            _, gt, mask, image_path = dataset.get_item(i)\n            prediction = self.model.infer(image_path)\n            \n            if mask.sum() == 0: continue\n            \n            val_pred = prediction[mask]\n            val_gt = gt[mask]\n            \n            if len(val_gt) > 5000:\n                val_pred = val_pred[::100]\n                val_gt = val_gt[::100]\n\n            # Spearman Correlation (rank-based, robust to scale)\n            corr, _ = spearmanr(val_pred, val_gt)\n            correlations.append(corr)\n        \n        avg_corr = np.mean(correlations)\n        \n        # If correlation is negative, values decrease as distance increases -> Disparity\n        return avg_corr < -0.1\n    \n    def __basic_benchmark(self, dataset, use_eigen_crop=False, debug=True):\n        absrel_list = []\n        delta_list = []\n        silog_list = []\n        \n        ds = \"ibims\"\n        if isinstance(dataset, NyuLoader):\n            ds = \"nyu\"\n\n        needs_inversion = self.__check_if_disparity(dataset)\n        if debug and needs_inversion:\n            print(f\"\\tModel outputs Disparity (negative correlation). Inverting predictions (1/x)...\")\n        \n        for i in tqdm(range(len(dataset))):\n            _, gt, mask, image_path = dataset.get_item(i)\n            prediction = self.model.infer(image_path)\n            \n            if use_eigen_crop:\n                # Standard Eigen Crop (Top, Bottom, Left, Right)\n                # [45:471, 41:601] is the valid region for 640x480 images\n                height, width = gt.shape\n                crop_mask = np.zeros((height, width), dtype=bool)\n                \n                y1, y2 = 45, min(471, height)\n                x1, x2 = 41, min(601, width)\n                crop_mask[y1:y2, x1:x2] = True\n                \n                mask = mask & crop_mask\n\n            # Skip invalid masks\n            if mask.sum() == 0: continue\n            \n            if self.relative:\n                if needs_inversion:\n                    gt_disparsity = np.zeros_like(gt)\n                    gt_disparsity[mask] = 1.0 / gt[mask]\n                    aligned_disp, _, _ = align_depth_least_square(gt_disparsity, prediction, mask)\n                    aligned_disp = np.maximum(aligned_disp, 1e-6)\n                    pred_final = 1.0 / aligned_disp\n                else:\n                    aligned_depth, _, _ = align_depth_least_square(gt, prediction, mask)\n                    pred_final = np.maximum(aligned_depth, 1e-6)\n            else:\n                pred_final = prediction\n\n            gt_valid = gt[mask]\n            pred_valid = pred_final[mask]\n            absrel = get_absrel(gt_valid, pred_valid)\n            delta = get_delta(gt_valid, pred_valid, 1)\n            silog = get_silog(gt_valid, pred_valid)\n            \n            absrel_list.append(absrel)\n            delta_list.append(delta)\n            silog_list.append(silog)\n            \n        am = np.mean(absrel_list)\n        dm = np.mean(delta_list)\n        sm = np.mean(silog_list)\n        self.set_results(ds, am, dm, sm)\n\n        if debug:\n            print(f\"\\tAverage AbsRel = {am:.4f}\")\n            print(f\"\\tAverage Delta1 = {dm:.4f}\")\n            print(f\"\\tAverage SILog  = {sm:.4f}\")\n\n    def benchmark(self, debug=True):\n        print(f\"{'='*20}[ {self.display_name} ]{'='*20}\")\n        if self.ibims is not None:\n            print(\"IBims-1 dataset:\")\n            self.__basic_benchmark(self.ibims, debug=debug)\n        if self.nyu is not None:\n            print(\"NYU Depth V2 dataset:\")\n            self.__basic_benchmark(self.nyu, use_eigen_crop=True, debug=debug)\n        print(f\"{'='*20}[ {self.display_name} ]{'='*20}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T22:00:20.802566Z","iopub.execute_input":"2026-01-09T22:00:20.803281Z","iopub.status.idle":"2026-01-09T22:00:21.947097Z","shell.execute_reply.started":"2026-01-09T22:00:20.803250Z","shell.execute_reply":"2026-01-09T22:00:21.946516Z"}},"outputs":[{"name":"stdout","text":"Found 654 RGB candidates. Matching with Depth...\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"configs = [\n    ModelConfig(\"depth-anything/Depth-Anything-V2-Large-hf\", \"DAV2-Large\"),\n    ModelConfig(\"depth-anything/Depth-Anything-V2-Metric-Indoor-Large-hf\", \"DAV2-Indoor-Metric\", relative=False),\n    ModelConfig(\"Intel/zoedepth-nyu-kitti\", \"ZoeDepth\", relative=False),\n    ModelConfig(\"Intel/dpt-large\", \"MiDaS-3.0\"),\n    ModelConfig(\"Intel/dpt-beit-large-512\", \"MiDaS-3.1\"),\n    ModelConfig(\"prs-eth/marigold-depth-v1-1\", \"Marigold-1.1\")\n]\n\nfor config in configs:\n    config.benchmark(debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T22:14:42.020635Z","iopub.execute_input":"2026-01-09T22:14:42.021384Z","iopub.status.idle":"2026-01-09T22:26:32.730096Z","shell.execute_reply.started":"2026-01-09T22:14:42.021353Z","shell.execute_reply":"2026-01-09T22:26:32.729279Z"}},"outputs":[{"name":"stdout","text":"found zoedepth model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"551da421d86d4c4c935b67a8f8fe31f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/942 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ad48470a46c435c84df2dc5763d8ac5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.37G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0be00e5cc724b1f91fabd56e0553278"}},"metadata":{}},{"name":"stderr","text":"Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/426 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9bcfc1661ae4e199914e55da84a6eb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dcc31ffabb5498ea12b92bff4cb65d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.38G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3972bfbd1f6446f08caeffa0f760d40d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_index.json:   0%|          | 0.00/534 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85a3b41d20c449c8a83d4fd7fc8f91b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"127827913439477ebb0f5faa54e260aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17a5b4afbcfe41218104882b14114088"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler_config.json:   0%|          | 0.00/533 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed42e08e2d8f4846956ed89aa2b3bda2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/824 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58658b8808df4317a668598e875d94ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"text_encoder/model.fp16.safetensors:   0%|          | 0.00/681M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbabae3200e64dffa3ced64f6111b987"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/460 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2018a1e700d54018907aa2a9576b59ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/633 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e3cf00d0a924975983c9bf370f39ba7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45a9c60c41274b19bb6160c6e85ac97a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unet/diffusion_pytorch_model.fp16.safete(…):   0%|          | 0.00/1.73G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ae5db2b123c4d7caad1b6c943005307"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f7904f2574243019c104e4bcc77871b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/611 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b270c4e1317c41999132a79c8c3c1635"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vae/diffusion_pytorch_model.fp16.safeten(…):   0%|          | 0.00/167M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b01721895214e0999c710e40b885b64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90fe8296fa3047c7bd63296a396f5845"}},"metadata":{}},{"name":"stdout","text":"found marigold model\n====================[ DAV2-Large ]====================\nIBims-1 dataset:\n\tModel outputs Disparity (negative correlation). Inverting predictions (1/x)...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:38<00:00,  2.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"\tAverage AbsRel = 0.1335\n\tAverage Delta1 = 0.9783\n\tAverage SILog  = 7.0373\nNYU Depth V2 dataset:\n\tModel outputs Disparity (negative correlation). Inverting predictions (1/x)...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:36<00:00,  2.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"\tAverage AbsRel = 0.0541\n\tAverage Delta1 = 0.9644\n\tAverage SILog  = 8.5579\n====================[ DAV2-Large ]====================\n\n====================[ DAV2-Indoor-Metric ]====================\nIBims-1 dataset:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:37<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"\tAverage AbsRel = 0.1265\n\tAverage Delta1 = 0.8861\n\tAverage SILog  = 7.5893\nNYU Depth V2 dataset:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:35<00:00,  2.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"\tAverage AbsRel = 0.2196\n\tAverage Delta1 = 0.6744\n\tAverage SILog  = 9.5989\n====================[ DAV2-Indoor-Metric ]====================\n\n====================[ ZoeDepth ]====================\nIBims-1 dataset:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:29<00:00,  3.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"\tAverage AbsRel = 0.2009\n\tAverage Delta1 = 0.6119\n\tAverage SILog  = 16.8612\nNYU Depth V2 dataset:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:27<00:00,  3.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"\tAverage AbsRel = 0.1464\n\tAverage Delta1 = 0.8267\n\tAverage SILog  = 17.2433\n====================[ ZoeDepth ]====================\n\n====================[ MiDaS-3.0 ]====================\nIBims-1 dataset:\n\tModel outputs Disparity (negative correlation). Inverting predictions (1/x)...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:17<00:00,  5.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"\tAverage AbsRel = 53.1020\n\tAverage Delta1 = 0.9413\n\tAverage SILog  = 12.1237\nNYU Depth V2 dataset:\n\tModel outputs Disparity (negative correlation). Inverting predictions (1/x)...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:14<00:00,  6.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"\tAverage AbsRel = 0.1028\n\tAverage Delta1 = 0.9007\n\tAverage SILog  = 13.6986\n====================[ MiDaS-3.0 ]====================\n\n====================[ MiDaS-3.1 ]====================\nIBims-1 dataset:\n\tModel outputs Disparity (negative correlation). Inverting predictions (1/x)...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:41<00:00,  2.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"\tAverage AbsRel = 1.0422\n\tAverage Delta1 = 0.9661\n\tAverage SILog  = 8.9308\nNYU Depth V2 dataset:\n\tModel outputs Disparity (negative correlation). Inverting predictions (1/x)...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:38<00:00,  2.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"\tAverage AbsRel = 0.0521\n\tAverage Delta1 = 0.9694\n\tAverage SILog  = 8.1113\n====================[ MiDaS-3.1 ]====================\n\n====================[ Marigold-1.1 ]====================\nIBims-1 dataset:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [02:40<00:00,  1.61s/it]\n","output_type":"stream"},{"name":"stdout","text":"\tAverage AbsRel = 0.0543\n\tAverage Delta1 = 0.9677\n\tAverage SILog  = 8.1879\nNYU Depth V2 dataset:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [02:38<00:00,  1.58s/it]","output_type":"stream"},{"name":"stdout","text":"\tAverage AbsRel = 0.0702\n\tAverage Delta1 = 0.9425\n\tAverage SILog  = 9.9901\n====================[ Marigold-1.1 ]====================\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11}]}