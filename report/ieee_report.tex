\documentclass[conference]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage{xurl}

% --- 1. Font and Language Setup for XeLaTeX ---
\usepackage{fontspec}
\usepackage{polyglossia}

% Set the default language to Greek, secondary to English
\setmainlanguage{greek}
\setotherlanguage{english}

% --- 2. Font Selection ---
% IEEE requires Times New Roman. On Linux/XeLaTeX, "TeX Gyre Termes" 
% is the standard free clone included in TeX Live.
\setmainfont{FreeSerif} % Times New Roman clone (for main text)
\setsansfont{TeX Gyre Heros} % Helvetica clone (for sans-serif spots)
\setmonofont{TeX Gyre Cursor} % Courier clone (for code)

% Polyglossia requires a specifically defined "greek font" family, 
% even if it is the same as the main font.
\newfontfamily\greekfont[Script=Greek]{FreeSerif}
\newfontfamily\greekfontsf[Script=Greek]{FreeSans}
\newfontfamily\greekfonttt[Script=Greek]{FreeMono}

% --- Robust Fix for Greek Labels in IEEEtran/XeLaTeX ---
\usepackage{etoolbox} % Required for the hook command \gappto

% Hook these changes into the Greek caption list so they stick
\AtBeginDocument{
  \renewcommand{\IEEEkeywordsname}{Λέξεις Κλειδιά}
  \renewcommand{\figurename}{Σχήμα}
  \renewcommand{\tablename}{Πίνακας}
  \renewcommand{\abstractname}{Περίληψη}
}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Benchmarking Monocular Depth Estimation Models}

\author{\IEEEauthorblockN{Νικόλας Σπυρόπουλος}
\IEEEauthorblockA{\textit{Σχολή Ηλεκτρολόγων Μηχανικών και Μηχανικών Υπολογιστών} \\
\textit{Εθνικό Μετσόβιο Πολυτεχνείο}\\
Αθήνα, Ελλάδα \\
el21202@mail.ntua.gr}}

\maketitle

\renewcommand{\refname}{Αναφορες}

\begin{abstract}
    Η εκτίμηση βάθους από μία εικόνα (Monocular Depth Estimation) αποτελεί θεμελιώδες πρόβλημα στην όραση υπολογιστή, με κρίσιμες εφαρμογές στην αυτόνομη πλοήγηση και την τρισδιάστατη ανακατασκευή. Η παρούσα εργασία εστιάζει στην πρόκληση της γενίκευσης τομέα (domain generalization), εξετάζοντας την εξέλιξη τεσσάρων σύγχρονων μοντέλων: MiDaS, ZoeDepth, Depth-Anything-V2 και Marigold. Αναλύονται οι αρχιτεκτονικές και οι στρατηγικές εκπαίδευσης που επιτρέπουν σε αυτά τα δίκτυα να αποδίδουν σε άγνωστες σκηνές, από τη μίξη ετερογενών δεδομένων του MiDaS και τον συνδυασμό σχετικού-μετρικού βάθους του ZoeDepth, έως τη χρήση συνθετικών δεδομένων στο Depth-Anything-V2 και την αξιοποίηση μοντέλων διάχυσης (diffusion models) στο Marigold. Στην συνέχεια, πραγματοποιείται πειραματική αξιολόγηση (benchmarking) των μοντέλων σε συνθήκες μηδενικής εκμάθησης (zero-shot). Για την αξιολόγηση χρησιμοποιήθηκαν τρία ετερογενή σύνολα δεδομένων: το NYU Depth V2 (εσωτερικοί χώροι), το KITTI (εξωτερικοί χώροι/αυτόνομη οδήγηση) και το iBims-1 (για έλεγχο ακρίβειας σε ακμές και διαφανείς επιφάνειες). Η μεθοδολογία περιλαμβάνει την ευθυγράμμιση προβλέψεων (scale alignment) για μοντέλα σχετικού βάθους και τη χρήση μετρικών όπως AbsRel, $\delta_1$ και SILog. Τα αποτελέσματα δείχνουν την υπεροχή των θεμελιωδών μοντέλων (foundation models) όπως το Depth-Anything-V2 στη γενίκευση, ενώ αναδεικνύουν τους περιορισμούς παλαιότερων προσεγγίσεων.
\end{abstract}

\begin{IEEEkeywords}
Monocular Depth Estimation, Benchmarking, Deep Learning, Computer Vision, Transformers
\end{IEEEkeywords}

\section{Εισαγωγη}
Η κατανόηση της τρισδιάστατης δομής ενός σκηνικού αποτελεί θεμελιώδη στόχο του τομέα της Όρασης Υπολογιστή (Computer Vision), με εφαρμογές στην αυτόνομη οδήγηση, τη ρομποτική πλοήγηση, την επαυξημένη πραγματικότητα (AR), την τρισδιάστατη ανακατασκευή και την ανάλυση σκηνών. Ένα από τα πιο σημαντικά προβλήματα που συνδέονται με αυτή την κατανόηση είναι η \textbf{εκτίμηση βάθους (Depth Estimation)}, δηλαδή ο υπολογισμός της απόστασης κάθε σημείου της εικόνας (pixel) από την κάμερα.

Τα τελευταία χρόνια, έχουν αναπτυχθεί μοντέλα εκτίμησης βάθους που μπορούν να παράγουν ακριβείς χάρτες βάθους ακόμη και από μία μόνο RGB εικόνα. Η συγκεκριμένη εφαρμογή ονομάζεται \textbf{Monocular Depth Estimation} καθώς δεν χρειάζεται δεδομένα από πολλούς αισθητήρες ή πολλές κάμερες, παρά μόνο μια εικόνα. Στο πλαίσιο αυτής της εργασίας, εξετάζονται τέσσερα σύγχρονα μοντέλα —\textbf{MiDaS}, \textbf{ZoeDepth}, \textbf{Depth-Anything-V2} και \textbf{Marigold}— τα οποία αντιπροσωπεύουν διαφορετικές αρχιτεκτονικές, τεχνικές μάθησης και επίπεδα ικανότητας στη γενίκευση μεταξύ τομέων (\textbf{domain generalization}). Αφού πρώτα, αναλυθούν οι αρχιτεκτονικές και οι ιδιαιτερότητες τους, θα γίνει σύγκριση της ακρίβειας των μοντέλων χρησιμοποιώντας ευρέως διαδεδομένα σύνολα δεδομένων(datasets).

Πριν παρουσιαστούν τα μοντέλα, είναι απαραίτητο να δοθούν οι βασικές έννοιες της εκτίμησης βάθους και τα είδη της, καθώς και το πρόβλημα της γενίκευσης τομέα, το οποίο αποτελεί κρίσιμο σημείο για την επιτυχία των μοντέλων Monocular Depth Estimation.

\section{Εκτιμηση Βαθους (Depth Estimation)}
Η εκτίμηση βάθους στοχεύει στην εξαγωγή ενός \textbf{χάρτη βάθους (depth map)}, στον οποίο για κάθε pixel αντιστοιχεί μια εκτίμηση της απόστασής του από την κάμερα. Παραδοσιακές προσεγγίσεις στηρίζονται σε στερεοσκοπικά συστήματα ή σε αισθητήρες ενεργού βάθους (όπως LiDAR ή ToF). Ωστόσο, η δυνατότητα πρόβλεψης βάθους από μόνο μια οπτική μιας εικόνας (monocular depth estimation) είναι ιδιαίτερα ελκυστική λόγω του χαμηλού κόστους, της ευκολίας ενσωμάτωσης και της ευρείας διαθεσιμότητας συστημάτων που διαθέτουν μόνο μία κάμερα.

Η εκτίμηση βάθους από μία μόνο εικόνα αποτελεί μη-σαφώς ορισμένο πρόβλημα (\textbf{ill-posed}), καθώς άπειρες τρισδιάστατες σκηνές μπορούν να προβάλουν την ίδια δισδιάστατη εικόνα. Γι’ αυτό, τα σύγχρονα μοντέλα βασίζονται σε μεγάλο βαθμό στη μάθηση στατιστικών και γεωμετρικών προτύπων από δεδομένα \cite{zhang2025surveymonocularmetricdepth}.

Η εκτίμηση βάθους διακρίνεται σε δύο βασικές υποκατηγορίες\footnote{\url{https://huggingface.co/docs/transformers/tasks/monocular_depth_estimation}}: \textbf{Απόλυτη (Absolute/Metric)} και \textbf{Σχετική (Relative)} εκτίμηση βάθους.

\subsection{Απόλυτη Εκτίμηση Βάθους (Absolute Depth Estimation)}
Η απόλυτη εκτίμηση βάθους αναφέρεται στην πρόβλεψη της πραγματικής απόστασης κάθε σημείου της σκηνής από την κάμερα, σε φυσικές μονάδες όπως μέτρα. Σε αυτή την περίπτωση, το μοντέλο πρέπει να εκτιμήσει όχι μόνο το σχήμα και τη δομή της σκηνής, αλλά και την κλίμακα της, ώστε ο χάρτης βάθους να ανταποκρίνεται ακριβώς στην πραγματικότητα. Η εκπαίδευση τέτοιων μοντέλων απαιτεί δεδομένα με μετρικές ετικέτες, συνήθως από αισθητήρες όπως LiDAR, structured light ή Time-of-Flight. Εξαιτίας αυτής της εξάρτησης από συγκεκριμένα συστήματα λήψης για ακριβείς μετρήσεις, η απόλυτη εκτίμηση βάθους είναι ευαίσθητη σε αλλαγές τομέα, όπως διαφορετικές κάμερες, συνθήκες φωτισμού ή διαφορετικούς τύπους σκηνών(πχ εσωτερικές ή εξωτερικές σκηνές). Παρ' όλα αυτά, αποτελεί αναντικατάστατη προσέγγιση σε εφαρμογές όπου η πραγματική απόσταση είναι κρίσιμη, όπως η αυτόνομη οδήγηση, η ρομποτική πλοήγηση και η τρισδιάστατη χαρτογράφηση.

\subsection{Σχετική Εκτίμηση Βάθους (Relative Depth Estimation)}
Η σχετική εκτίμηση βάθους επικεντρώνεται στην πρόβλεψη της δομής και της τοπολογίας της σκηνής, χωρίς να επιδιώκει να εκφράσει τις αποστάσεις σε πραγματικές μονάδες. Ο χάρτης βάθους που παράγει ένα τέτοιο μοντέλο είναι σε κλίμακα η οποία δεν έχει κάποια φυσική σημασία, αποτυπώνοντας όμως με συνέπεια το ποια σημεία βρίσκονται πιο κοντά ή πιο μακριά από την κάμερα. Επειδή δεν απαιτείται μετρικό ground truth, η σχετική εκτίμηση βάθους μπορεί να εκπαιδευτεί σε πολύ μεγάλα, ετερογενή σύνολα δεδομένων, συχνά προερχόμενα από διαφορετικές πηγές ή με ποικιλία σκηνών. Αυτό καθιστά τη μέθοδο ιδιαίτερα ανθεκτική σε αλλαγές τομέα, με αποτέλεσμα να επιτυγχάνει καλύτερη γενίκευση σε νέες συνθήκες, διαφορετικούς τύπους σκηνών ή διαφορετικούς αισθητήρες. Για τον λόγο αυτό, πολλά σύγχρονα μοντέλα που δίνουν έμφαση στην ευρεία γενίκευση υιοθετούν τη σχετική εκτίμηση βάθους, ιδιαίτερα όταν η ακριβής κλίμακα δεν αποτελεί προϋπόθεση για την εκάστοτε εφαρμογή.

\section{Γενικευση Τομεα (Domain Generalization)}
Η γενίκευση τομέα αποτελεί ένα από τα βασικά ζητήματα στη εκτίμηση βάθους από μία εικόνα και αναφέρεται στην ικανότητα ενός μοντέλου να διατηρεί υψηλή απόδοση όταν εφαρμόζεται σε δεδομένα που διαφέρουν από αυτά στα οποία εκπαιδεύτηκε. Ένα μοντέλο που δεν γενικεύει καλά μπορεί να λειτουργεί άριστα σε ένα συγκεκριμένο σύνολο δεδομένων, αλλά να παρουσιάζει σημαντική πτώση στην απόδοση όταν αντιμετωπίζει διαφορετικές σκηνές, συνθήκες φωτισμού ή κάμερες.

Η αντιμετώπιση του προβλήματος της γενίκευσης τομέα απαιτεί ειδικές στρατηγικές, όπως η εκπαίδευση σε ποικιλόμορφα και πολυδιάστατα datasets, η χρήση τεχνικών κανονικοποίησης και scale-invariant loss functions \cite{eigen2014depthmappredictionsingle}, καθώς και η αξιοποίηση μεγάλων συνόλων συνθετικών \cite{Yao_2024} και πραγματικών δεδομένων. Όλα τα μοντέλα που εξετάζονται στην εργασία —MiDaS, ZoeDepth, Depth-Anything-V2 και Marigold— αντιμετωπίζουν το ζήτημα της γενίκευσης με διαφορετικούς τρόπους, στοχεύοντας στην επίτευξη αξιόπιστης απόδοσης σε ποικίλες συνθήκες και σκηνές, ανεξαρτήτως των δεδομένων εκπαίδευσης.

\section{MiDaS (Mixing Datasets for Zero-shot Cross-dataset Transfer)}
Το MiDaS αντιπροσωπεύει μια κομβική εξέλιξη στην εκτίμηση βάθους από μία μόνο εικόνα, καθώς αντιμετωπίζει συστηματικά το κρίσιμο πρόβλημα της γενίκευσης τομέα. Η επιτυχία του μοντέλου έγκειται στην ανάπτυξη μεθοδολογιών για την αποτελεσματική \textbf{σύνθεση ετερογενών συνόλων δεδομένων (mixing datasets)}, ακόμη και αν οι αρχικές τους ετικέτες βάθους είναι διαφορετικού τύπου (inconsistent). Οι πρώτες δύο εκδόσεις του MiDaS χρησιμοποιούσαν συνελικτικά νευρωνικά δίκτυα (Convolutional Neural Networks - CNNs) \cite{ranftl2020robustmonoculardepthestimation}, ενώ η τρίτη και τελευταία έκδοση υιοθετεί διαφορετικές αρχιτεκτονικές Vision Transfer (ViT) \cite{ranftl2021visiontransformersdenseprediction} ακολουθώντας τις εξελίξεις στο πεδίο της όρασης υπολογιστή. 

\subsection{Αρχιτεκτονική}
Η αρχιτεκτονική του MiDaS εξελίχθηκε σε δύο κύριες φάσεις, διατηρώντας πάντα τη βασική δομή \textbf{Κωδικοποιητή-Αποκωδικοποιητή (Encoder-Decoder)}.

\subsubsection{MiDaS v1 \& v2 (CNN-based)}
Οι αρχικές εκδόσεις του MiDaS βασίστηκαν στα \textbf{Συνελικτικά Νευρωνικά Δίκτυα (CNNs)}, τα οποία ήταν η κυρίαρχη τεχνολογία για την όραση υπολογιστή την εποχή της πρώτης δημοσίευσης \cite{ranftl2020robustmonoculardepthestimation}. 
\begin{itemize}
    \item \textbf{Κωδικοποιητής (Encoder)}: Χρησιμοποιήθηκε ένα δίκτυο υψηλής χωρητικότητας τύπου \textbf{ResNeXt-101-WSL (Weakly-Supervised Learning)}. Η επιλογή ενός τόσο βαθιού δικτύου και η προ-εκπαίδευση σε μαζικά, αδύναμα επιβλεπόμενα δεδομένα (WSL) επέτρεψε στον κωδικοποιητή να μάθει εξαιρετικά ισχυρές και γενικεύσιμες αναπαραστάσεις χαρακτηριστικών (Transfer Learning), απαραίτητες για την επιτυχία του zero-shot transfer. Στην αρχική δημοσίευση αναφέρεται επίσης ότι και με χρήση ενός μικρότερου κωδικοποιητή, όπως το \textbf{ResNet-50}, το μοντέλο κατάφερε να επιτύχει καλύτερη απόδοση από τα τότε state-of-the-art μοντέλα.
    \item \textbf{Αποκωδικοποιητής / Κεφαλή προβλέψεων (Decoder / Prediction Head)}: Ο αποκωδικοποιητής ήταν υπεύθυνος για την ανασύνθεση του χάρτη βάθους από τα αφηρημένα χαρακτηριστικά του κωδικοποιητή. Χρησιμοποιούσε τεχνικές \textbf{multi-scale feature fusion}\footnote{\url{https://medium.com/@nbeel.original/getting-started-with- depth-estimation-using-midas-and-python-d0119bfe1159}} μέσω συνδέσεων παράκαμψης (Skip Connections), για να συνδυάσει λεπτομερή χωρικά χαρακτηριστικά (από ρηχά στρώματα) με εννοιολογικές πληροφορίες (από βαθιά στρώματα), επαναφέροντας την ανάλυση στην αρχική διάσταση της εικόνας μέσω \textbf{ανοδικής δειγματοληψίας (Upsampling)}.

\end{itemize}

\subsubsection{MiDaS v3.0 \& v3.1 / DPT (Transformer-Based)}
Οι νεότερες εκδόσεις του MiDaS (συχνά αναφερόμενες ως \textbf{DPT - Dense Prediction Transformer}) διατήρησαν τη μεθοδολογία εκπαίδευσης, αλλά αντικατέστησαν τον CNN κωδικοποιητή με ένα \textbf{Vision Transformer (ViT)} \cite{ranftl2021visiontransformersdenseprediction}.

\begin{itemize}
    \item \textbf{Κωδικοποιητής (Encoder)}: Υιοθετήθηκαν διάφοροι ViT backbones, όπως ViT, BEIT, Swin και SwinV2. Η επιτυχία τους οφείλεται στην ικανότητά τους να μοντελοποιούν μακροχρόνιες εξαρτήσεις (long-range dependencies) σε όλη την εικόνα, καταγράφοντας αποτελεσματικότερα την ολική γεωμετρία (global structure) της σκηνής. Αυτό βελτιώνει περαιτέρω τη γενίκευση σε σκηνές με άγνωστη δομή.
    \item \textbf{Αποκωδικοποιητής (Decoder)}: Η αρχιτεκτονική του αποκωδικοποιητή προσαρμόστηκε για να λαμβάνει ως είσοδο τα "\textbf{tokens}" (τα διακριτά τμήματα πληροφορίας) από τα διάφορα στάδια του Transformer. Στην συνέχεια "συναρμολογεί" εκ νέου (\textbf{reassemble}) αυτά τα tokens σε αναπαραστάσεις εικόνας πολλαπλών αναλύσεων, τις οποίες επεξεργάζεται για να παραχθεί ο τελικός πυκνός χάρτης βάθους.
\end{itemize}

\subsection{Εκπαίδευση}
Η στρατηγική εκπαίδευσης του MiDaS είναι η κύρια καινοτομία του και παραμένει σταθερή σε όλες τις εκδόσεις, εξασφαλίζοντας την αρχιτεκτονική ανεξαρτησία της μεθόδου. Σχεδιάστηκε για να επιτύχει \textbf{Zero-shot Cross-dataset Transfer}, δηλαδή το μοντέλο αξιολογείται σε ένα test dataset το οποίο δεν είναι υποσύνολο του train dataset. 

Το μοντέλο εκπαιδεύτηκε συνδυάζοντας δεδομένα από \textbf{πολλαπλά και ετερογενή σύνολα (datasets)}, όπως ReDWeb, MegaDepth, DIML Indoor, WSVD και, κυρίως, μια νέα, μαζική πηγή από καρέ 3D ταινιών (3D Movies). Αυτή η ανάμειξη εξασφαλίζει ότι το μοντέλο εκτίθεται σε τεράστια ποικιλία σκηνών (πχ Indoor/Outdoor, Static/Dynamic), καθιστώντας τις αναπαραστάσεις που προσπαθεί να μάθει ανθεκτικές και γενικές. Χρησιμοποιήθηκε η τεχνική \textbf{Pareto-optimal Multi-objective Optimization (Βελτιστοποίηση Πολλαπλών Στόχων)} η οποία αντιμετωπίζει την εκπαίδευση σε κάθε dataset ως ξεχωριστό στόχο, εξασφαλίζοντας ισορροπημένη μάθηση ώστε η βελτίωση της απόδοσης σε ένα dataset να μην υποβαθμίζει την απόδοση σε κάποιο άλλο (φαινόμενο που παρατηρήθηκε σε πείραμα εκπαίδευσης με μια \textbf{αφελή - naive μέθοδο}).

Για να καταστεί δυνατή η συνεκπαίδευση σε datasets με ασύμβατες ετικέτες (πχ μετρικό βάθος έναντι σχετικού βάθους), το MiDaS παράγει τις προβλέψεις του στον χώρο της \textbf{Δυσαναλογίας (Disparity Space)} (δηλαδή του αντίστροφου βάθους, $\text{D}^{-1}$), ο οποίος είναι αριθμητικά σταθερός και συμβατός με τις όλες τις πηγές των ground truths σε όλα τα σύνολα δεδομένων. Για να το καταφέρει αυτό εισάγει μια καινοτόμο συνάρτηση απώλειας (Loss function). Χρησιμοποιείται η \textbf{Scale- and Shift-Invariant Trimmed MAE} ($\mathcal{L}_{ssitrim}$) η οποία είναι αδιάφορη ως προς την κλίμακα ($s$) και τη μετατόπιση ($t$) (scale- and shift-invariant), επιτρέποντας την εκπαίδευση σε ανομοιογενείς ετικέτες. Επιπλεόν, είναι ανθεκτική (robust), καθώς αποκόπτει (trims) το 20\% των ακραίων τιμών (outliers) σε κάθε εικόνα, μειώνοντας την ευαισθησία του μοντέλου σε μη ακριβή ετικέτες του ground truth.

Τέλος, η επιτυχία του MiDaS εξαρτάται και από τις τεχνικές \textbf{Transfer Learning} που εφαρμόστηκαν. Χρησιμοποιήθηκαν encoders υψηλής χωρητικότητας (π.χ., ResNeXt-101-WSL ή Vision Transformers) που είχαν προ-εκπαιδευτεί σε τεράστια σύνολα δεδομένων (π.χ., ImageNet ή Weakly-Supervised Data) πριν από την εκπαίδευση με στόχο την εκτίμηση του βάθους. Αυτή η προ-εκπαίδευση παρέχει στον κωδικοποιητή εξαιρετικά γενικεύσιμες αναπαραστάσεις των χαρακτηριστικών της εικόνας, οι οποίες μεταφέρονται στην εργασία εκτίμησης βάθους, ενισχύοντας την ικανότητα του μοντέλου να ερμηνεύει άγνωστες σκηνές.

\section{ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth}
Το \textbf{ZoeDepth} αντιπροσωπεύει εξέλιξη της έρευνας του μοντέλου MiDaS. Ενώ τα MiDaS v1-v3 ήταν εξαιρετικά στην πρόβλεψη του σχετικού βάθους (relative depth) –δηλαδή τη σωστή διάταξη των αντικειμένων στο χώρο–, απέτυχαν να διατηρήσουν τη μετρική κλίμακα (metric scale) (πχ το πραγματικό βάθος σε μέτρα). Το ZoeDepth είναι η πρώτη προσέγγιση που συνδυάζει την γενίκευση του MiDaS με την ικανότητα διατήρησης της μετρικής κλίμακας \cite{bhat2023zoedepthzeroshottransfercombining}.

\subsection{Αρχιτεκτονική}
Το ZoeDepth διατηρεί την αρχιτεκτονική Κωδικοποιητή-Αποκωδικοποιητή, κληρονομώντας τις βέλτιστες πρακτικές του DPT. Προκειμένου να αντιμετωπίσει το μέχρι τότε δυσεπίλυτο πρόβλημα της απόλυτης εκτίμησης βάθους, εισάγει μια κρίσιμη καινοτομία στην κεφαλή πρόβλεψης, το \textbf{Metric Bins Module}.
\begin{itemize}
    \item \textbf{Κωδικοποιητής}: Όπως ακριβώς και το MiDaS v3, το ZoeDepth χρησιμοποιεί ViT ως backbone και συγκεκριμένα το \textbf{Beit-L}. Έτσι, όπως έχει αποδειχθεί, εξασφαλίζει την καλύτερη δυνατή γενίκευση και μοντελοποίηση των ολικών εξαρτήσεων στην εικόνα.
    \item \textbf{Αποκωδικοποιητής}: Ο αποκωδικοποιητής παραμένει παρόμοιος με αυτόν του DPT, αλλά με μια σημαντική καινοτομία - το \textbf{Metric Bins Module (MBM)}. Αντί να προβλέπει το βάθος ως μία συνεχή τιμή (regression), το ZoeDepth προβλέπει πιθανότητες βάθους σε ένα σύνολο \textbf{διακριτών bins}, τα οποία αναπαριστούν ένα συγκεκριμένο εύρος βάθους (πχ 0-80m για εξωτερικές σκηνές ή 0-10m για εσωτερικές). Το MBM επιτρέπει την προσαρμογή της θέσης αυτών των bins κατά τη διάρκεια της εκπαίδευσης με μετρικό βάθος (\textbf{metric depth fine-tuning}).
    \item \textbf{Router}: Επειδή το μοντέλο έχει γίνει fine-tune σε διαφορετικά datasets για εσωτερικές και εξωτερικές σκηνές και υπάρχουν επομένως, διαφορετικές κεφαλές (\textbf{Metric Heads}) για κάθε τύπο σκηνής. Έτσι, είναι αναγκαίο να υπάρχει ένας μηχανισμός που να επιλέγει την κατάλληλη κεφαλή ανάλογα με την είσοδο. Για αυτό το σκοπό, το ZoeDepth εισάγει έναν \textbf{Router}, ο οποίος αποτελείται από ένα \textbf{Multi Layer Perceptron (MLP)} ταξινομητή που εκπαιδεύεται ταυτόχρονα με το υπόλοιπο μοντέλο.
\end{itemize}

\subsection{Εκπαίδευση}
Η εκπαίδευση του ZoeDepth αναλύεται σε δύο φάσεις. Στην πρώτη φάση γίνεται ένα pre-train εμπνευσμένο από το MiDaS, ενώ στην δεύτερη φάση γίνεται το fine-tune που επιλύει το πρόβλημα της απόλυτης/μετρικής εκτίμησης βάθους.

\subsubsection{Φάση 1: Προ-εκπαίδευση Σχετικού Βάθους (Relative Depth Pre-training)}
Η πρώτη φάση ακολουθεί την  μεθοδολογία του MiDaS. Εκπαιδεύεται σε 12 datasets με ετικέτες \textbf{σχετικού βάθους} χρησιμοποιώντας την ίδια συνάρτηση απώλειας \textbf{Scale- and Shift-Invariant Loss}. Έτσι, χτίζεται ένας ισχυρά γενικεύσιμος κωδικοποιητής, ικανός να ερμηνεύει ποικίλα datasets.

\subsubsection{Φάση 2: Fine-tuning Μετρικού Βάθους (Metric Depth Fine-tuning)}
Η δεύτερη φάση εκπαίδευσης είναι μια διαδικασία fine-tuning, με στόχο την ευθυγράμμιση του ήδη γενικεύσιμου κωδικοποιητή με την πραγματική μετρική κλίμακα. Κατά τη διάρκεια αυτής της φάσης, το μοντέλο χρησιμοποιεί δύο υψηλής ποιότητας datasets που περιέχουν μετρικό ground truth -το \textbf{KITTI} για εξωτερικές σκηνές και το \textbf{NYU Depth V2} για εσωτερικές.

Ουσιαστικά, η εκπαίδευση επικεντρώνεται στο Metric Bins Module (MBM), την νέα κεφαλή του μοντέλου. Ενώ στην πρώτη φάση το μοντέλο έμαθε να τοποθετεί τα αντικείμενα σωστά σε σχετικό βάθος, στη δεύτερη φάση το MBM ενεργοποιείται και βελτιστοποιείται. Το μοντέλο μαθαίνει να προσαρμόζει τις εκπαιδεύσιμες παραμέτρους που ορίζουν τα όρια των διακριτών metric bins, ώστε να αντιστοιχούν ακριβέστερα στις πραγματικές μετρικές τιμές.

Χρησιμοποιώντας μια τυπική μετρική συνάρτηση απώλειας (metric loss), το μοντέλο μαθαίνει την σωστή κλίμακα, επιτυγχάνοντας πλέον ακριβή πρόβλεψη βάθους σε μέτρα. Το αποτέλεσμα είναι ένα μοντέλο που διατηρεί την εξαιρετική ικανότητα zero-shot transfer που αποκτήθηκε από την προ-εκπαίδευση, ενώ ταυτόχρονα μπορεί να δίνει αξιόπιστες μετρικές τιμές σε διαφορετικού είδους εικόνες εισόδου.

\section{Depth-Anything-V2}
Το Depth Anything V2 (DA-V2) αποτελεί ένα σημαντικό βήμα προς την επίτευξη πιο \textbf{λεπτών (finer-grained)} και πιο \textbf{ανθεκτικών (robust)} προβλέψεων σχετικού βάθους. Η καινοτομία του δεν έγκειται σε περίπλοκες τεχνικές, αλλά σε μερικές πρακτικές που αφορούν την κλιμάκωση των δεδομένων και τη διαδικασία εκπαίδευσης \cite{depth_anything_v1} \cite{depth_anything_v2}.

\subsection{Αρχιτεκτονική}
Η αρχιτεκτονική του DA-V2 ακολουθεί το πρότυπο Κωδικοποιητή-Αποκωδικοποιητή που καθιερώθηκε από τα DPT/MiDaS, δίνοντας έμφαση στη χρήση ενός Vision Transformer για τη μέγιστη ικανότητα γενίκευσης.

\begin{itemize}
    \item \textbf{Κωδικοποιητής}: Χρησιμοποιείται ένας μεγάλος Vision Transformer (ViT), ο \textbf{ViT-Large} που περιέχει 335M παραμέτρους. Είναι προ-εκπαιδευμένος μέσω ενός σχήματος \textbf{αυτο-επιβλεπόμενης μάθησης (Self Supervised Learning)} σύμφωνα με την μέθοδο που ορίζει το \textbf{DINOv2} \cite{oquab2024dinov2learningrobustvisual}. Η συγκεκριμένη μέθοδος επιτρέπει στους Transformers να μαθαίνουν ισχυρές, υψηλής ποιότητας αναπαραστάσεις χωρίς ανθρώπινη επισήμανση (labels), καθιστώντας τον κωδικοποιητή εξαιρετικά αποτελεσματικό στην εξαγωγή πλούσιων γεωμετρικών χαρακτηριστικών που απαιτούνται για την εκτίμηση βάθους. Τέλος, μέσω του σχήματος Teacher-Student, αφού δημιουργηθεί το μεγάλο μοντέλο \textbf{Teacher} με τον ViT-Large, η έρευνα δίνει μεγάλη έμφαση στο πώς μπορεί να μεταφερθεί αυτή η γνώση (\textbf{Knowledge Distillation}) σε μικρότερα \textbf{Student} μοντέλα, πχ ViT-Small και ViT-Base, εμφανώς λιγότερων παραμέτρων.
    \item \textbf{Αποκωδικοποιητής}: Η δομή του αποκωδικοποιητή είναι παρόμοια με αυτή του DPT. Eίναι υπεύθυνος για τη σύντηξη (fusion) των χαρακτηριστικών πολλαπλών επιπέδων (multi-scale features) που εξάγονται από τον κωδικοποιητή ViT. Χρησιμοποιεί \textbf{ανοδική δειγματοληψία (upsampling)} και συνελικτικές μονάδες για να ανακτήσει τις χωρικές λεπτομέρειες και να παράξει τον τελικό πυκνό χάρτη βάθους, διατηρώντας την ανάλυση και τις λεπτές ακμές των αντικειμένων.
\end{itemize}

\subsection{Εκπαίδευση}
Η εκπαίδευση του Depth-Anything-V2 διαφέρει ριζικά από τα προηγούμενα μοντέλα, καθώς απορρίπτει όλα τα datasets με πραγματικές μετρικές ετικέτες (labeled real images) και βασίζεται σε μια διαδικασία τριών βημάτων που εστιάζει στα \textbf{συνθετικά δεδομένα} και στη \textbf{Μετάδοση Γνώσης}.

\subsubsection{Βήμα 1: Αποκλειστική χρήση Συνθετικών Δεδομένων (Synthetic Data Training)}
Το DA-V2 αντικατέστησε όλες τις επισημασμένες πραγματικές εικόνες με συνθετικές εικόνες. Το \textbf{Teacher Model} εκπαιδεύτηκε αποκλειστικά σε ένα μαζικό σύνολο συνθετικών δεδομένων (BlendedMVS, TartanAir, HRWSI, κ.ά.), τα οποία παρέχουν τέλειο και καθαρό ground truth βάθους, χωρίς τα σφάλματα και τους θορύβους των πραγματικών αισθητήρων (πχ LiDAR). Αυτή η στρατηγική επιτρέπει στο μοντέλο να μάθει τις βασικές γεωμετρικές αρχές με μεγάλη ακρίβεια.

\subsubsection{Βήμα 2: Παραγωγή Ψευδο-ετικετών (Pseudo-Labeling)}
Το μοντέλο που εκπαιδεύτηκε στα συνθετικά δεδομένα (Teacher Model) χρησιμοποιείται για να \textbf{παράγει ψευδο-ετικέτες (pseudo-labels)} σε μεγάλης κλίμακας πραγματικές εικόνες που δεν έχουν ετικέτες (unlabeled real images). Με αυτόν τον τρόπο, το μοντέλο γεφυρώνει το χάσμα μεταξύ των συνθετικών και των πραγματικών δεδομένων. Οι ψευδο-ετικέτες διατηρούν την υψηλή ακρίβεια των γεωμετρικών κανόνων που μάθανε από τα συνθετικά, αλλά εφαρμόζονται στην πραγματική κατανομή εικόνων του κόσμου (real-world distribution).

\subsubsection{Βήμα 3: Μετάδοση Γνώσης}
Η Μετάδοση Γνώσης είναι η διαδικασία με την οποία η γνώση που αποκτήθηκε από το μεγάλο μοντέλο Teacher μεταφέρεται σε ένα μικρότερο και πιο αποδοτικό μοντέλο Student. Ουσιαστικά, το Teacher Model, το οποίο έχει ήδη μάθει από συνθετικά δεδομένα και έχει δημιουργήσει ψευδο-ετικέτες σε μαζικές, μη επισημασμένες πραγματικές εικόνες, χρησιμοποιείται για να "διδάξει" το Student Model (πχ, ViT-Small, 25M παραμέτρων). Το Student εκπαιδεύεται με μια πιο "χαλαρή" συνάρτηση απώλειας ώστε να μιμηθεί την πρόβλεψη βάθους του Teacher. Αυτή η μέθοδος επιτρέπει στο Student να διατηρεί την υψηλή ακρίβεια, την ανθεκτικότητα και την ικανότητα παραγωγής λεπτών λεπτομερειών του Teacher, επιτυγχάνοντας παράλληλα πολύ \textbf{ταχύτερη εξαγωγή (inference speed)} και σημαντικά \textbf{μειωμένο αριθμό παραμέτρων}. Με αυτόν τον τρόπο, το Depth-Anything-V2 μπορεί να προσφέρει μοντέλα κατάλληλα για \textbf{εφαρμογές πραγματικού χρόνου}.

\section{Marigold: Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation}
Το \textbf{Marigold} σηματοδοτεί μια κομβική και ριζική αλλαγή στην προσέγγιση της εκτίμησης βάθους, απομακρύνοντας την έρευνα από τα παραδοσιακά ViT/CNN μοντέλα (MiDaS, ZoeDepth, Depth Anything) που βασίζονται σε encoders οπτικών χαρακτηριστικών. Η καινοτομία του Marigold έγκειται στην αξιοποίηση της εκτεταμένης οπτικής γνώσης για τον φυσικό κόσμο που έχει αποκτηθεί από ένα προ-εκπαιδευμένο μοντέλο διάχυσης (\textbf{pre-trained Diffusion Model}), όπως το \textbf{Stable Diffusion}. Έτσι χρησιμοποιεί παίρνει αυτό το ισχυρό generative μοντέλο εικόνας και το κάνει \textbf{fine-tune για την εργασία της εκτίμησης βάθους} \cite{ke2024repurposingdiffusionbasedimagegenerators}.

\subsection{Αρχιτεκτονική}
Η αρχιτεκτονική του Marigold είναι θεμελιωδώς διαφορετική από τα προηγούμενα μοντέλα, καθώς βασίζεται στην αξιοποίηση ενός προ-εκπαιδευμένου \textbf{Μοντέλου Διάχυσης (Diffusion Model)}, του Stable Diffusion, αντί να εκπαιδευτεί από την αρχή ένα Vision Transformer. Συγκεκριμένα, βασίζεται στο \textbf{Noise Predictor} του Stable Diffusion, το οποίο είναι ένα \text{U-Net} δίκτυο.

\begin{itemize}
    \item Η ακρίβεια του Marigold δεν προέρχεται από την εκπαίδευση σε δεδομένα βάθους, αλλά από την πλούσια οπτική κατανόηση της δομής και της σύνθεσης των σκηνών που έχει αποκτήσει το U-Net από την προ-εκπαίδευσή του σε \textbf{"internet-scale"} πλήθος εικόνων (πχ στο σύνολο δεδομένων LAION-5B).
    \item \textbf{Ρόλος του U-Net}: Η αρχιτεκτονική του U-Net είναι σχεδιασμένη για πυκνές χωρικές εξαρτήσεις, καθιστώντας το κατάλληλο για την εργασία της εκτίμησης βάθους, ακόμα και αν ο αρχικός του ρόλος ήταν η πρόβλεψη θορύβου.
    \item \textbf{Είσοδος}: Το δίκτυο τροποποιείται ώστε να δέχεται την \textbf{RGB εικόνα} της σκηνής, μαζί με τον προστιθέμενο θόρυβο και την Τιμή Χρόνου ($t$) από τη διαδικασία διάχυσης.
    \item \textbf{Έξοδος}: Η τελική έξοδος του U-Net είναι ένας \textbf{χάρτης δυσαναλογίας (disparity map)}, ο οποίος στη συνέχεια επεξεργάζεται για να δώσει το τελικό βάθος.
\end{itemize}

\subsection{Εκπαίδευση}
Η εκπαίδευση του Marigold εστιάζει στο fine-tuning της ήδη υπάρχουσας γνώσης, ενώ η πρόβλεψη αξιοποιεί τον επαναληπτικό μηχανισμό της διάχυσης.

\begin{itemize}
    \item \textbf{Fine-tuning σε συνθετικά δεδομένα}: Το μοντέλο δεν εκπαιδεύεται σε μαζικά, ετερογενή datasets βάθους (όπως το MiDaS), αλλά χρησιμοποιεί έναν σχετικά μικρό αριθμό συνθετικών δεδομένων, περίπου 74K εικόνες, ώστε να "μάθει" να παράγει ακριβείς χάρτες βάθους. Ο σκοπός είναι η "μετάφραση" της ήδη κωδικοποιημένης οπτικής γνώσης (από το Stable Diffusion) στον κανόνα αντιστοίχισης μεταξύ RGB εικόνας και βάθους. Αυτό το πετυχαίνει χρησιμοποιώντας απλές τεχνικές κανονικοποίησης (regularization), όπως L1/L2 απώλειες.
    \item \textbf{Επαναληπτική Σύνθεση (Iterative Ensembling)}: Κατά την φάση της \textbf{πρόβλεψης (inference)}, το Marigold διατηρεί τον επαναληπτικό χαρακτήρα του diffusion model και αντί για μόνο ένα πέρασμα, εκτελείται για 10 \textbf{βήματα δειγματοληψίας (sampling steps)}. Έτσι, ο τελικός χάρτης βάθους προκύπτει από την \textbf{σύνθεση (ensembling)} όλων των επαναληπτικών προβλέψεων, με αποτέλεσμα την μεγάλη βελτίωση της ποιότητας και της ανθεκτικότητας (robustness) της πρόβλεψης.
\end{itemize}

\section{Αναλυτικη Περιγραφη Μεθοδολογιας}
Έχοντας πλέον καλύτερη γνώση τόσο των αρχιτεκτονικών όσο και των τρόπων εκπαίδευσης των μοντέλων, πριν παρουσιαστεί η πειραματική μέθοδος για την σύγκριση και αξιολόγησή τους, είναι αναγκαίο να γίνει σαφής η επιλογή των συνόλων δεδομένων (\textit{"datasets"}) που χρησιμοποιήθηκαν. Η αξιολόηση αυτή έγινε σε συνθήκες \textit{"zero-shot"}, δηλαδή δεν προηγήθηκε καμία διαδικασία \textit{"fine-tuning"} ή επεξεργασία των αποτελεσμάτων, ώστε να αναδειχθούν οι ικανότητες ή οι περιορισμοί των μοντέλων σαν να χρησιμοποιούνταν για οποιαδήποτε εικόνα σε οποιοδήποτε πλαίσιο (\textit{"in-the-wild"}). Χρησιμοποιήθηκαν εικόνες από τρία διαφορετικά σύνολα δεδομένων\footnote{\url{https://www.kaggle.com/datasets/patiencechewyeecheah/ibims-1}} \footnote{\url{https://www.kaggle.com/datasets/soumikrakshit/nyu-depth-v2}} \footnote{\url{https://www.kaggle.com/datasets/artemmmtry/kitti-depth-prediction-evaluation}} ώστε να φανεί η ικανότητα κάθε μοντέλου στην γενίκευση τομέα.

\subsection{Επιλογή Συνόλων Δεδομένων}
Για την κάλυψη ενός μεγάλου φάσματος σκηνών και την αξιολόγηση της ευρωστίας (robustness) των μοντέλων, επιλέχθηκαν τρία ετερογενή σύνολα δεδομένων (datasets). Λόγω του μεγάλου αριθμού δεδομένων που περιείχαν τα τελυεταία δύο σύνολα και του περιορισμένου αριθμού πόρων, επέλεξα τυχαία 100 δείγματα (sampling) από κάθε ένα και έτσι κάθε σύνολο δεδομένων για την αξιολόγηση περιέχει 100 εικόνες.

\subsection{IBims-1}
Το σύνολο δεδομένων \textbf{IBims-1} \cite{Koch18:ECS} επιλέχθηκε ως ένα εξειδικευμένο εργαλείο αξιολόγησης για σκηνές εσωτερικού χώρου, εστιάζοντας στην ποιότητα των λεπτομερειών και την ορθότητα των ακμών. Σε αντίθεση με τα τυπικά datasets εκπαίδευσης, το IBims-1 περιέχει σκηνές με με διαφανείς επιφάνειες και έντονες μεταβάσεις βάθους, επιτρέποντας τον έλεγχο της συμπεριφοράς των μοντέλων σε δύσκολες συνθήκες. Για την παρούσα εργασία χρησιμοποιήθηκε το υποσύνολο "core" (περιέχει 100 σκηνές), χωρίς περαιτέρω χωρική περικοπή (cropping), προκειμένου να εξεταστεί η ικανότητα των αλγορίθμων να διαχειρίζονται ολόκληρη την πληροφορία της σκηνής και να διατηρούν τη δομική συνοχή σε αντικείμενα με περίπλοκα όρια ακόμα και στις άκρες της κάθε εικόνας.

\subsection{NYU Depth V2}
Το σύνολο δεδομένων \textbf{NYU Depth V2} \cite{Silberman:ECCV12} αποτελεί ένα καθιερωμένο πρότυπο αναφοράς για την εκτίμηση βάθους σε εσωτερικούς χώρους, περιλαμβάνοντας πυκνούς χάρτες βάθους (dense depth maps) και εικόνες RGB από ποικίλες οικιακές και επαγγελματικές σκηνές ληφθείσες με αισθητήρα Microsoft Kinect. Ο κύριος σκοπός της χρήσης του είναι η συγκριτική αξιολόγηση της γενίκευσης των μοντέλων σε τυπικά περιβάλλοντα εσωτερικού χώρου. Στο πλαίσιο της πειραματικής διαδικασίας, εφαρμόστηκε κατά την αξιολόγηση η τεχνική Eigen Crop, κάνοντας χρήση της μάσκας εγκυρότητας που παρέχεται με το dataset. Η επεξεργασία αυτή είναι απαραίτητη για να εξαιρεθούν από τον υπολογισμό του σφάλματος τα pixels στα όρια της εικόνας όπου ο αισθητήρας δεν παρέχει έγκυρες μετρήσεις ή υπάρχουν παραμορφώσεις προβολής.

\subsection{ΚΙΤΤΙ}
Για την αξιολόγηση σε εξωτερικούς χώρους χρησιμοποιήθηκε το σύνολο δεδομένων \textbf{KITTI} \cite{Geiger2013IJRR}, το οποίο παρέχει αραιούς χάρτες βάθους (sparse depth maps) προερχόμενους από αισθητήρες LiDAR Velodyne. Σκοπός της χρήσης του είναι η εξέταση της απόδοσης των μοντέλων σε μεγάλες αποστάσεις και σε δυναμικά περιβάλλοντα με έντονες φωτιστικές αλλαγές. Λόγω της διάταξης των αισθητήρων στο όχημα καταγραφής, οι μετρήσεις στο πάνω μέρος (ουρανός) και στα πλάγια της εικόνας είναι συχνά αναξιόπιστες. Για τον λόγο αυτό, εφαρμόστηκε η τυπική διαδικασία περικοπής Garg Crop, η οποία περιορίζει την περιοχή αξιολόγησης στο κεντρικό τμήμα της εικόνας όπου τα δεδομένα ground truth είναι πυκνά και αξιόπιστα, διασφαλίζοντας έτσι μια δίκαιη σύγκριση για τα μοντέλα που δεν έχουν εκπαιδευτεί ειδικά σε αυτό το πεδίο.

\subsection{Προεπεξεργασία και Ευθυγράμμιση}
Μια σημαντική πρόκληση στη σύγκριση διαφορετικών μοντέλων είναι ότι ορισμένα (π.χ. MiDaS) παράγουν σχετικό βάθος (relative depth) χωρίς φυσικές μονάδες, ενώ άλλα (π.χ. ZoeDepth) παράγουν μετρικό βάθος (metric depth). Για να γίνει δυνατή και δίκαιη η σύγκριση μεταξύ των δύο κατηγοριών μοντέλων ακολουθήθηκε η εξής διαδικασία:
\begin{itemize}
    \item Για μοντέλα \textbf{σχετικού βάθους}: Εφαρμόζεται ευθυγράμμιση κλίμακας και μετατόπισης (Scale and Shift Alignment) με τη μέθοδο των Ελαχίστων Τετραγώνων (Least Squares), ώστε η πρόβλεψη να προσαρμοστεί στο εύρος τιμών του ground truth: 
    \[
    D_{aligned} = sD_{pred} + t
    \]
    όπου $s$ και $t$ είναι οι παράμετροι κλίμακας και μετατόπισης.
    \item Για μοντέλα \textbf{μετρικού βάθους}: Οι προβλέψεις αξιολογούνται απευθείας, χωρίς καμία προσαρμογή, καθώς στόχος είναι να ελεγχθεί η ικανότητά τους να εκτιμούν τις πραγματικές αποστάσεις.
    \item Χειρισμός \textbf{Disparity}: Για μοντέλα που εξάγουν disparity (αντιστρόφως ανάλογο του βάθους) αντί για depth map, εφαρμόζεται αντιστροφή ($1/x$) πριν την αξιολόγηση.
\end{itemize}

\subsection{Μετρικές Αξιολόγησης}
Για την ποσοτική αποτίμηση των αποτελεσμάτων χρησιμοποιήθηκαν τρεις καθιερωμένες μετρικές στη βιβλιογραφία της εκτίμησης βάθους. Για όλες τις παρακάτω μετρικές έστω $d_i$ η τιμή βάθους του ground truth pixel $i$, $\hat{d}_i$ η προβλεπόμενη τιμή από το μοντέλο και $N$ ο συνολικός αριθμός των έγκυρων pixels (έγκυρα pixels θεωρούνται όσα δεν έχουν αποβληθεί μέσω των μασκών (masks) που δίνονται από κάθε σύνολο δεδομένων).
\begin{itemize}
    \item Απόλυτο Σχετικό Σφάλμα (Absolute Relative Error - AbsRel): δίνεται από τον τύπο
    \[
    AbsRel = \frac{1}{N} \sum_{i=1}^{N} \frac{|d_i - \hat{d}_i|}{d_i}
    \]
    To \textbf{AbsRel} εκφράζει το σφάλμα ως ποσοστό της πραγματικής απόστασης. Είναι σημαντική μετρική γιατί για παράδειγμα ένα σφάλμα 50 εκατοστών είναι αμελητέο αν το αντικείμενο βρίσκεται στα 50 μέτρα, αλλά καταστροφικό αν το αντικείμενο βρίσκεται στο 1 μέτρο. Έτσι, κανονικοποιεί το σφάλμα ώστε να έχει την ίδια βαρύτητα ανεξάρτητα από την απόσταση. Όσο πιο κοντά είναι η τιμή του στο 0, τόσο καλύτερη είναι η επίδοση του μοντέλου.
    \item Ακρίβεια Δέλτα: δίνεται από τον τύπο
    \[
    \delta_1 = \frac{1}{N} \sum_{i=1}^{N} 1(max(\frac{d_i}{\hat{d}_i},\frac{\hat{d}_i}{d_i})<1.25)
    \]
    Το \textbf{$\delta_1$} αναδεικνύει το ποσοστό των "επιτυχημένων" pixels. Ένα pixel θεωρείται σωστό ή επιτυχημένο αν η πρόβλεψη δεν απέχει περισσότερο από 25\% από την πραγματική τιμή. Είναι ένας δείκτης της αξιοπιστίας (robustness) του μοντέλου. Δεν δίνει βάρος στο πόσο μεγάλο είναι το λάθος στα αποτυχημένα pixels (πχ outliers), αλλά πόσο συχνά το μοντέλο "πέφτει μέσα". Επομένως, η τιμή του πρέπει να είναι όσο πιο κοντά στο 1 γίνεται για να θεωρείται καλό το μοντέλο βάσει αυτής της μετρικής.
    \item Λογαριθμικό Σφάλμα Ανεξαρτήτου Κλίμακας (Scale Invariant Logarithmic Error - SILog): Έστω $\Delta_i = log\delta_i - log\hat{\delta}_i$ και ο συντελεστής βάρους $\lambda=1$. Δίνεται από τον τύπο
    \[
    SILog = \sqrt{\frac{1}{N} \sum_{i=1}^{N} \Delta_i^2 - \frac{\lambda}{N^2} (\sum_{i=1}^{N} \Delta_i)^2} \times 100
    \]
    Το \textbf{SILog} μετράει τη γεωμετρική συνέπεια της σκηνής, αγνοώντας το απόλυτο μέγεθος. Πολλά μοντέλα (ειδικά τα relative depth models) μπορεί να καταλάβουν σωστά ότι ένα αντικείμενο βρίσκεται μπροστά από ένα άλλο, αλλά να κάνουν λάθος στο μέγεθος ολόκληρης της σκηνής. Έτσι, το SILog "τιμωρεί" τα λάθη στις σχέσεις των αντικειμένων (δομικά λάθη), αλλά "συγχωρεί" το μοντέλο αν έχει κάνει λάθος στην γενική κλίμακα (scale) όλης της εικόνας. Όσο πιο κοντά είναι η τιμή του στο 0, τόσο καλύτερα έχει κατανοήσει το μοντέλο την τριδιάστατη δομή της σκηνής.
\end{itemize}

\subsection{Περιγραφή πειραματικής διαδικασίας}
Η πειραματική διαδικασία σχεδιάστηκε ως ένα ενιαίο pipeline αξιολόγησης για να διασφαλιστεί η δίκαιη και συνεπής σύγκριση όλων των μοντέλων. Η ανάπτυξη και εκτέλεση των πειραμάτων πραγματοποιήθηκε η πλατφόρμα που παρέχει το Kaggle σε περιβάλλον Python με χρήση των βιβλιοθηκών transformers και diffusers. Από τις επιλογές σε hardware που προσφέρει το Kaggle ελεύθερα, αξιοποιήθηκε η κάρτα γραφικών (GPU) NVIDIA Tesla P100 (16GB VRAM), στοιχείο απαραίτητο για την αποδοτική εκτέλεση (inference) των βαρύτερων μοντέλων όπως το Marigold.

Όλα τα checkpoints για τα μοντέλα που χρησιμοποιήθηκαν είναι από το Hugging Face και πιο συγκεκριμένα:
\begin{itemize}
    \item \textbf{DAV2-Large}\footnote{\url{https://huggingface.co/depth-anything/Depth-Anything-V2-Large-hf}} (relative depth): Είναι το βασικό μοντέλο Depth Anything V2 όπως παρουσιάστηκε και στο θεωρητικό κομμάτι. Είναι το μεγαλύτερο από τα διαθέσιμα μοντέλα ώστε να αξιολογηθεί το "καλύτερο" από τα όλα τα διαθέσιμα.
    \item \textbf{DAV2-Indoor-Metric}\footnote{\url{https://huggingface.co/depth-anything/Depth-Anything-V2-Metric-Indoor-Large-hf}} (metric depth): Το backbone του είναι το Depth Anything V2 αλλά έχει γίνει fine-tune σε μετρικά δεδομένα εσωτερικών χώρων.
    \item \textbf{DAV2-Outdoor-Metric}\footnote{\url{https://huggingface.co/depth-anything/Depth-Anything-V2-Metric-Outdoor-Large-hf}} (metric depth): Παρόμοια, έχει backbone το κλασσικό Depth Anything V2 και έχει γίνει fine-tune σε μετρικά δεδομένα εξωτερικών χώρων.
    \item \textbf{ZoeDepth}\footnote{\url{https://huggingface.co/Intel/zoedepth-nyu-kitti}} (metric depth): Έχει ως πυρήνα το κλασσικό ZoeDepth από το αρχικό paper και έχει γίνει fine-tune στα σύνολα Nyu και KITTI.
    \item \textbf{MiDaS-3.0}\footnote{\url{https://huggingface.co/Intel/dpt-large}} (relative depth): Η έκδοση 3.0 του MiDaS.
    \item \textbf{MiDaS-3.1}\footnote{\url{https://huggingface.co/Intel/dpt-beit-large-512}} (relative depth): Η έκδοση 3.1 του MiDaS με τον Beit transformer ως backbone.
    \item \textbf{Marigold-1.1}\footnote{\url{https://huggingface.co/prs-eth/marigold-depth-v1-1}} (relative depth): Η έκδοση 1.1 του Marigold. Χρησιμοποιήθηκαν μόνο 4 βήματα αποθορυβοποίησης (denoising steps) λόγω των περιορισμένων πόρων.
\end{itemize}

Για κάθε ζεύγος εικόνας εισόδου $I$ και χάρτη βάθους αναφοράς (Ground Truth - $D_{gt}$) από τα σύνολα δεδομένων δοκιμής, ακολουθήθηκε η εξής διαδικασία:

\begin{itemize}
    \item Προσαρμογή Εισόδου (Input Resizing): Η εικόνα RGB αναδιατάσσεται (resize) στις διαστάσεις που απαιτεί η αρχιτεκτονική του εκάστοτε μοντέλου (π.χ. $518 \times 518$ για το Depth-Anything-V2), διατηρώντας την αναλογία διαστάσεων όπου αυτό είναι εφικτό.
    
    \item Εκτέλεση Μοντέλου (Inference): Η εικόνα τροφοδοτείται στο μοντέλο και λαμβάνεται στην έξοδο η πρόβλεψη του μοντέλου είτε ως depth map είτε ως disparity map.
    
    \item Ανάκτηση Ανάλυσης (Resolution Restoration): Η παραγόμενη πρόβλεψη βάθους $D_{pred}$ επαναφέρεται στην αρχική ανάλυση της εικόνας εισόδου ($H \times W$) μέσω διγραμμικής παρεμβολής (bilinear interpolation), ώστε να υπάρχει αντιστοίχιση pixel-προς-pixel με το Ground Truth.
    
    \item Διαχείριση Τιμών (Value Handling):
    \begin{itemize}
        \item Εάν το μοντέλο παράγει disparity, οι τιμές αντιστρέφονται ($1/x$).
        \item Εάν το μοντέλο παράγει σχετικό βάθος, εφαρμόζεται η ευθυγράμμιση ελαχίστων τετραγώνων (scale and shift alignment) που περιγράφηκε προηγουμένως.
    \end{itemize}
    
    \item Εφαρμογή Μάσκας (Masking \& Cropping): Δημιουργείται μια δυαδική μάσκα εγκυρότητας $M$, η οποία προκύπτει από την τομή των έγκυρων pixels του Ground Truth και της περιοχής ενδιαφέροντος που ορίζει το κάθε dataset (Eigen Crop για το NYU, Garg Crop για το KITTI).
    
    \item Υπολογισμός Σφαλμάτων: Οι μετρικές αξιολόγησης υπολογίζονται αποκλειστικά για τα pixels που ανήκουν στη μάσκα $M$, αγνοώντας τις περιοχές χωρίς πληροφορία ή τις περιοχές που έχουν εξαιρεθεί λόγω της διαδικασίας περικοπής.
\end{itemize}

\section{Πειραματικα Αποτελεσματα}

\section{Συμπερασματα}

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}